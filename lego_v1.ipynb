{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in some tweets\n",
    "import csv\n",
    "jack_reader = csv.reader(open('data/realdonaldtrump.csv', 'r'))\n",
    "\n",
    "columns = next(jack_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jack_tweets = list(jack_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank you Reno, Nevada. \n",
      "NOTHING will stop us in our quest to MAKE AMERICA SAFE AND GREAT AGAIN! #AmericaFirst… https://t.co/A4eeHoCbGS\n",
      "Join me live in Reno, Nevada!\n",
      "https://t.co/T4bf1hrxaA https://t.co/EPqRXHa1CM\n",
      "JOIN ME TOMORROW!\n",
      "MINNESOTA • 2pm\n",
      "https://t.co/WcgLh4prS7\n",
      "\n",
      "MICHIGAN • 6pm\n",
      "https://t.co/9BqGVKNNrt\n",
      "\n",
      "VIRGINIA • 9:30p… https://t.co/A1oVhCrT6t\n",
      "#DrainTheSwamp!\n",
      "https://t.co/z68vGp9Bvf\n",
      "Top Clinton Aides Bemoan Campaign ‘All Tactics,’ No Vision: https://t.co/mHYvQtIq78\n",
      "‘Must Act Immediately’: Clinton Charity Lawyer Told Execs They Were Breaking The Law\n",
      "https://t.co/hsi4qhqTV1\n",
      "Watch Coach Mike Ditka- a great guy and supporter tonight at 8pmE on #WattersWorld with @jessebwatters @FoxNews.\n",
      "Thank you Wilmington, North Carolina. We are 3 days away from the CHANGE you've been waiting for your entire life!… https://t.co/6ZJZRBfLST\n",
      "Thank you for the incredible support this morning Tampa, Florida! #ICYMI- watch here: https://t.co/q43kHf7MoE https://t.co/1GscFNaV4L\n"
     ]
    }
   ],
   "source": [
    "# Filter out retweets\n",
    "jack_tweets_no_rts = list(filter(lambda x: not x[1].startswith('RT'), jack_tweets))\n",
    "\n",
    "for i in range(10):\n",
    "    print(jack_tweets_no_rts[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> \n",
      "thank you reno, nevada. \n",
      "nothing will stop us in our quest to make america safe and great again! #americafirst… URL\n",
      "  -> thank you reno, nevada. \n",
      "nothing will stop us in our quest to make america safe and great again! #americafirst… URL\n",
      "join me live in reno, nevada!\n",
      "URL URL\n",
      "  -> join me live in reno, nevada!\n",
      "URL URL\n",
      "join me tomorrow!\n",
      "minnesota • 2pm\n",
      "URL\n",
      "\n",
      "michigan • 6pm\n",
      "URL\n",
      "\n",
      "virginia • 9:30p… URL\n",
      "  -> join me tomorrow!\n",
      "minnesota • 2pm\n",
      "URL\n",
      "\n",
      "michigan • 6pm\n",
      "URL\n",
      "\n",
      "virginia • 9:30p… URL\n",
      "#draintheswamp!\n",
      "URL\n",
      "  -> #draintheswamp!\n",
      "URL\n",
      "top clinton aides bemoan campaign ‘all tactics,’ no vision: URL\n",
      "  -> top clinton aides bemoan campaign ‘all tactics,’ no vision: URL\n",
      "‘must act immediately’: clinton charity lawyer told execs they were breaking the law\n",
      "URL\n",
      "  -> ‘must act immediately’: clinton charity lawyer told execs they were breaking the law\n",
      "URL\n",
      "watch coach mike ditka- a great guy and supporter tonight at 8pme on #wattersworld with @jessebwatters @foxnews.\n",
      "  -> watch coach mike ditka- a great guy and supporter tonight at 8pme on #wattersworld with USER USER.\n",
      "thank you wilmington, north carolina. we are 3 days away from the change you've been waiting for your entire life!… URL\n",
      "  -> thank you wilmington, north carolina. we are 3 days away from the change you've been waiting for your entire life!… URL\n",
      "thank you for the incredible support this morning tampa, florida! #icymi- watch here: URL URL\n",
      "  -> thank you for the incredible support this morning tampa, florida! #icymi- watch here: URL URL\n"
     ]
    }
   ],
   "source": [
    "# Canonicalize the tweet text as lowercase\n",
    "import re\n",
    "jack_tweets_no_rts_lowercase = [tweet[1].lower() for tweet in jack_tweets_no_rts]\n",
    "\n",
    "# Canonicalize links to \"URL\" and @mentions to \"USER\"\n",
    "jack_tweets_no_rts_lowercase = [re.sub(r'(https?:\\/\\/t\\.co\\/\\w+)', 'URL', tweet) for tweet in jack_tweets_no_rts_lowercase]\n",
    "jack_tweets_normalized = [re.sub(r'(@\\w+)', 'USER', tweet) for tweet in jack_tweets_no_rts_lowercase]\n",
    "\n",
    "for i in range(10):\n",
    "    print(jack_tweets_no_rts_lowercase[i])\n",
    "    print(\"  -> {}\".format(jack_tweets_normalized[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "# note: need to nltk.download() all the models the first time aroudn\n",
    "\n",
    "# Frequency distribution of words.\n",
    "main_dist = nltk.FreqDist([])\n",
    "for tweet in jack_tweets_normalized:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # We don't want to count the mentions and hashtags\n",
    "    # tokens = list(filter(lambda x: not (x[0] == '@' or x[0] == '#'), tokens))\n",
    "    \n",
    "    main_dist.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 1095),\n",
       " ('!', 972),\n",
       " ('URL', 875),\n",
       " (',', 863),\n",
       " ('the', 846),\n",
       " ('#', 753),\n",
       " ('to', 517),\n",
       " ('USER', 487),\n",
       " ('and', 468),\n",
       " ('in', 442),\n",
       " ('is', 363),\n",
       " ('of', 359),\n",
       " ('you', 351),\n",
       " (':', 351),\n",
       " ('a', 345),\n",
       " ('i', 295),\n",
       " ('will', 280),\n",
       " ('hillary', 276),\n",
       " ('for', 259),\n",
       " ('thank', 234),\n",
       " ('on', 229),\n",
       " ('``', 191),\n",
       " ('clinton', 191),\n",
       " ('-', 190),\n",
       " ('we', 190),\n",
       " ('at', 173),\n",
       " (\"''\", 172),\n",
       " ('great', 172),\n",
       " ('be', 163),\n",
       " ('that', 157),\n",
       " ('crooked', 151),\n",
       " ('me', 145),\n",
       " ('are', 143),\n",
       " ('it', 139),\n",
       " (\"'s\", 138),\n",
       " (';', 133),\n",
       " ('&', 132),\n",
       " ('amp', 131),\n",
       " ('with', 126),\n",
       " ('by', 117)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dist.most_common()[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hmac\n",
    "import binascii\n",
    "import struct\n",
    "from nltk import word_tokenize\n",
    "\n",
    "hash_key = \"legomystego\"\n",
    "\n",
    "def tweet_hash(tweet):\n",
    "    \"\"\"\n",
    "    Implement a keyed hash function according to section 3.3:\n",
    "    \n",
    "    1. Generate a keyed hash digest (HMAC-MD5) for each word\n",
    "    2. Get the last four bits of the hash\n",
    "    3. Bitwise rotate each value according to its position in the tweet\n",
    "    4. XOR all the values together\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    token_hashes = []\n",
    "    tweet_hash = 0     # Start with 0, the XOR identity\n",
    "    for n, token in enumerate(tokens):\n",
    "        # Generate the keyed hash with the given key\n",
    "        m = hmac.new(hash_key.encode(), msg=token.encode())\n",
    "        \n",
    "        m_hash = m.digest()\n",
    "        \n",
    "        # Get the last nibble of the hash\n",
    "        m_bits = int(m_hash[-1]) & 0x0f\n",
    "        \n",
    "        m_bits = rot_n(m_bits, n)\n",
    "        \n",
    "        # ROT-N for the position in the tweet\n",
    "        for i in range(n):\n",
    "            m_bits_shifted = m_bits << 1\n",
    "            m_bits_overflow = m_bits_shifted & 0xf0\n",
    "            m_bits_lower = m_bits_shifted & 0x0f\n",
    "            \n",
    "            m_bits = m_bits_lower + (m_bits_overflow >> 4)\n",
    "            \n",
    "        tweet_hash ^= m_bits\n",
    "        token_hashes.append(m_bits)\n",
    "        \n",
    "    return hex(tweet_hash)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0xd'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_hash(\"ferociosu world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_hash(\"world hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x6'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_hash(\"magical wonderful magic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Substitutions\n",
    "import sqlite3\n",
    "import dataset\n",
    "RULES_DATABASE_URI = \"sqlite:///ppdb/rules.db\"\n",
    "\n",
    "rules_database = dataset.connect(RULES_DATABASE_URI)\n",
    "lexical_rules = rules_database[\"lexical\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OrderedDict([('id', 57148), ('source', 'attraction'), ('target', 'attractiveness'), ('features', '1.0')])]\n"
     ]
    }
   ],
   "source": [
    "print(list(lexical_rules.find(source=\"attraction\")))\n",
    "rules = list(lexical_rules.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attractiveness']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_synonyms(rules, token):\n",
    "    \"\"\"\n",
    "    Get a list of synonyms for the given token\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    token: a string of the token for which to find synonyms\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    a list of synonyms\n",
    "    \"\"\"\n",
    "    return list(set([r['target'] for r in list(rules.find(source=token))]))\n",
    "\n",
    "# Test\n",
    "get_synonyms(lexical_rules, \"attraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_synonyms_for_list(rules, tokens):\n",
    "    \"\"\"\n",
    "    Generate synonyms for each token.\n",
    "    \n",
    "    Return as a dictionary\n",
    "    \"\"\"\n",
    "    synonyms = {}\n",
    "    \n",
    "    for token in tokens:\n",
    "        current_synonyms = get_synonyms(rules, token)\n",
    "        \n",
    "        if current_synonyms:\n",
    "            synonyms[token] = current_synonyms\n",
    "        \n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attractive': ['appealing'], 'fairly': ['equitably']}\n",
      "{'movie': ['film'], 'evening': ['night']}\n"
     ]
    }
   ],
   "source": [
    "print(get_synonyms_for_list(lexical_rules, \"I'm fairly attractive\".split()))\n",
    "print(get_synonyms_for_list(lexical_rules, \"What movie are you seeing this evening\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "model = kenlm.LanguageModel('data/tweets.klm')\n",
    "\n",
    "\n",
    "def get_possible_cover_tweet(rules, tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    synonyms = get_synonyms_for_list(rules, tokens)\n",
    "\n",
    "    # A list of tuples (score, cover tweet)\n",
    "    possibilities = []\n",
    "\n",
    "    for token, alternatives in synonyms.items():\n",
    "        for alt in alternatives:\n",
    "            alt_tweet = tweet.replace(token, alt)\n",
    "            alt_score = model.score(alt_tweet)\n",
    "            \n",
    "            possibilities.append((alt_score, alt_tweet, tweet_hash(alt_tweet)))\n",
    "                        \n",
    "    possibilities = sorted(possibilities, reverse=True)\n",
    "    \n",
    "    return possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possibilities = get_possible_cover_tweet(lexical_rules, \"what movie do you want to see this evening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-25.122745513916016, 'what film do you want to see this evening', '0xd'),\n",
       " (-25.782970428466797, 'what movie do you wanna to see this evening', '0xa'),\n",
       " (-26.328113555908203, 'what movie do you want to see this night', '0x4')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilities[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o2m_rules = rules_database[\"o2m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possibilities_o2m = get_possible_cover_tweet(o2m_rules, \"i am going to see grandma\")\n",
    "possibilities_lexical = get_possible_cover_tweet(lexical_rules, \"i am going to see grandma\")\n",
    "\n",
    "possibilities = set()\n",
    "possibilities.update(possibilities_o2m)\n",
    "possibilities.update(possibilities_lexical)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-10.450414657592773, 'i am going to see the section grandma', '0x4'),\n",
       " (-12.506351470947266, 'i am &apos;re going to see grandma', '0x5'),\n",
       " (-15.1917085647583, 'i am going to see your grandmother', '0xf'),\n",
       " (-15.006108283996582, 'maybe i am gomaybe ing to see grandma', '0x0'),\n",
       " (-9.867966651916504, 'i am going to see section grandma', '0x8')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilities = list(possibilities)\n",
    "\n",
    "possibilities[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9.151080131530762, 'i am going to see granny', '0x8'),\n",
       " (-9.867966651916504, 'i am going to see section grandma', '0x8'),\n",
       " (-10.068314552307129, 'i am going to refer to grandma', '0xd'),\n",
       " (-10.447188377380371, 'i am going to get it grandma', '0x0'),\n",
       " (-10.450414657592773, 'i am going to see the section grandma', '0x4'),\n",
       " (-10.454346656799316, 'i am going to consult with grandma', '0xa'),\n",
       " (-10.834543228149414, 'i am going to talk to grandma', '0x1'),\n",
       " (-10.943902969360352, 'i am going to look at grandma', '0x6'),\n",
       " (-11.232831001281738, 'i am &apos;re gonna to see grandma', '0xd'),\n",
       " (-11.232831001281738, 'i am &apos;m gonna to see grandma', '0xf')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = sorted(list(possibilities), key=lambda t:t[0], reverse=True)\n",
    "filtered[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: these are possibities within the  the language model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv Python 3.5 Stego",
   "language": "python",
   "name": "ipykernel-stego"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
